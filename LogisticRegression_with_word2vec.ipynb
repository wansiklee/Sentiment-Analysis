{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "LogisticRegression_with_word2vec.ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRTNivItE5wM",
        "colab_type": "text"
      },
      "source": [
        "# Word2Vec을 활용한 모델 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv0kgDIeE5wN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEZUqEMgE5wR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_IN_PATH = r'./data_in/'\n",
        "TRAIN_CLEAN_DATA = 'train_clean.csv'\n",
        "\n",
        "train_data = pd.read_csv(DATA_IN_PATH+TRAIN_CLEAN_DATA)\n",
        "\n",
        "reviews = list(train_data['review'])\n",
        "sentiment = list(train_data['sentiment'])\n",
        "\n",
        "sentences = []\n",
        "for review in reviews:\n",
        "    sentences.append(review.split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6lqShkVE5wT",
        "colab_type": "text"
      },
      "source": [
        "# word2vec 벡터화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEO_NgvUE5wV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 학습시 필요한 하이퍼 파라미터\n",
        "num_features = 300     # 각 단어에 대해 임베딩된 벡터의 차원\n",
        "min_word_count = 40    # 단어에 대한 최소 빈도 수\n",
        "num_workers = 4        # 학습을 위한 프로세스 개수\n",
        "context = 10           # word2vec을 수행하기 위한 컨텍스트 윈도우 크기\n",
        "downsampling = 1e-3    # 학습을 수행할 때 빠른 학습을 위한 정답 단어 라벨에 대한 다운샘플링 비율. 보통 0.001이 좋은 성능을 낸다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rht7QB5oE5wX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIiu7sSRE5wa",
        "colab_type": "code",
        "colab": {},
        "outputId": "be23fce0-49a2-4e61-b418-042cada12dc5"
      },
      "source": [
        "from gensim.models import word2vec\n",
        "print(\"Training model...\")\n",
        "model = word2vec.Word2Vec(sentences,\n",
        "                          workers=num_workers,\n",
        "                          size=num_features,\n",
        "                          min_count=min_word_count,\n",
        "                          window = context,\n",
        "                          sample = downsampling)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-07-31 12:39:06,995 : INFO : collecting all words and their counts\n",
            "2019-07-31 12:39:07,004 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-07-31 12:39:07,400 : INFO : PROGRESS: at sentence #10000, processed 1205223 words, keeping 51374 word types\n",
            "2019-07-31 12:39:07,727 : INFO : PROGRESS: at sentence #20000, processed 2396605 words, keeping 67660 word types\n",
            "2019-07-31 12:39:07,896 : INFO : collected 74065 word types from a corpus of 2988089 raw words and 25000 sentences\n",
            "2019-07-31 12:39:07,897 : INFO : Loading a fresh vocabulary\n",
            "2019-07-31 12:39:07,962 : INFO : effective_min_count=40 retains 8160 unique words (11% of original 74065, drops 65905)\n",
            "2019-07-31 12:39:07,963 : INFO : effective_min_count=40 leaves 2627273 word corpus (87% of original 2988089, drops 360816)\n",
            "2019-07-31 12:39:08,015 : INFO : deleting the raw counts dictionary of 74065 items\n",
            "2019-07-31 12:39:08,018 : INFO : sample=0.001 downsamples 30 most-common words\n",
            "2019-07-31 12:39:08,020 : INFO : downsampling leaves estimated 2494384 word corpus (94.9% of prior 2627273)\n",
            "2019-07-31 12:39:08,059 : INFO : estimated required memory for 8160 words and 300 dimensions: 23664000 bytes\n",
            "2019-07-31 12:39:08,062 : INFO : resetting layer weights\n",
            "2019-07-31 12:39:08,277 : INFO : training model with 4 workers on 8160 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
            "2019-07-31 12:39:09,296 : INFO : EPOCH 1 - PROGRESS: at 16.30% examples, 407596 words/s, in_qsize 8, out_qsize 0\n",
            "2019-07-31 12:39:10,317 : INFO : EPOCH 1 - PROGRESS: at 32.49% examples, 402576 words/s, in_qsize 8, out_qsize 0\n",
            "2019-07-31 12:39:11,326 : INFO : EPOCH 1 - PROGRESS: at 49.36% examples, 407971 words/s, in_qsize 8, out_qsize 0\n",
            "2019-07-31 12:39:12,330 : INFO : EPOCH 1 - PROGRESS: at 64.76% examples, 401036 words/s, in_qsize 6, out_qsize 1\n",
            "2019-07-31 12:39:13,332 : INFO : EPOCH 1 - PROGRESS: at 81.46% examples, 403486 words/s, in_qsize 8, out_qsize 0\n",
            "2019-07-31 12:39:14,340 : INFO : EPOCH 1 - PROGRESS: at 98.83% examples, 407317 words/s, in_qsize 4, out_qsize 0\n",
            "2019-07-31 12:39:14,349 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-07-31 12:39:14,370 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-07-31 12:39:14,390 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-07-31 12:39:14,397 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-07-31 12:39:14,399 : INFO : EPOCH - 1 : training on 2988089 raw words (2494588 effective words) took 6.1s, 407838 effective words/s\n",
            "2019-07-31 12:39:15,416 : INFO : EPOCH 2 - PROGRESS: at 15.33% examples, 384198 words/s, in_qsize 8, out_qsize 0\n",
            "2019-07-31 12:39:16,421 : INFO : EPOCH 2 - PROGRESS: at 32.49% examples, 406386 words/s, in_qsize 6, out_qsize 1\n",
            "2019-07-31 12:39:17,448 : INFO : EPOCH 2 - PROGRESS: at 49.68% examples, 411003 words/s, in_qsize 6, out_qsize 1\n",
            "2019-07-31 12:39:18,458 : INFO : EPOCH 2 - PROGRESS: at 67.36% examples, 417107 words/s, in_qsize 8, out_qsize 0\n",
            "2019-07-31 12:39:19,482 : INFO : EPOCH 2 - PROGRESS: at 84.64% examples, 417768 words/s, in_qsize 7, out_qsize 0\n",
            "2019-07-31 12:39:20,291 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-07-31 12:39:20,306 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-07-31 12:39:20,320 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-07-31 12:39:20,332 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-07-31 12:39:20,334 : INFO : EPOCH - 2 : training on 2988089 raw words (2494390 effective words) took 5.9s, 420861 effective words/s\n",
            "2019-07-31 12:39:21,356 : INFO : EPOCH 3 - PROGRESS: at 15.95% examples, 398707 words/s, in_qsize 8, out_qsize 0\n",
            "2019-07-31 12:39:22,359 : INFO : EPOCH 3 - PROGRESS: at 32.50% examples, 405967 words/s, in_qsize 8, out_qsize 0\n",
            "2019-07-31 12:39:23,367 : INFO : EPOCH 3 - PROGRESS: at 50.00% examples, 415824 words/s, in_qsize 8, out_qsize 0\n",
            "2019-07-31 12:39:24,407 : INFO : EPOCH 3 - PROGRESS: at 65.43% examples, 403273 words/s, in_qsize 8, out_qsize 0\n",
            "2019-07-31 12:39:25,446 : INFO : EPOCH 3 - PROGRESS: at 82.05% examples, 402411 words/s, in_qsize 8, out_qsize 0\n",
            "2019-07-31 12:39:26,478 : INFO : EPOCH 3 - PROGRESS: at 96.42% examples, 392611 words/s, in_qsize 7, out_qsize 1\n",
            "2019-07-31 12:39:26,607 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-07-31 12:39:26,620 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-07-31 12:39:26,639 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-07-31 12:39:26,651 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-07-31 12:39:26,653 : INFO : EPOCH - 3 : training on 2988089 raw words (2494453 effective words) took 6.3s, 395267 effective words/s\n",
            "2019-07-31 12:39:27,681 : INFO : EPOCH 4 - PROGRESS: at 15.95% examples, 396393 words/s, in_qsize 8, out_qsize 0\n",
            "2019-07-31 12:39:28,701 : INFO : EPOCH 4 - PROGRESS: at 32.82% examples, 405440 words/s, in_qsize 6, out_qsize 0\n",
            "2019-07-31 12:39:29,736 : INFO : EPOCH 4 - PROGRESS: at 49.36% examples, 403639 words/s, in_qsize 8, out_qsize 0\n",
            "2019-07-31 12:39:30,745 : INFO : EPOCH 4 - PROGRESS: at 64.00% examples, 393269 words/s, in_qsize 7, out_qsize 0\n",
            "2019-07-31 12:39:31,753 : INFO : EPOCH 4 - PROGRESS: at 81.13% examples, 398349 words/s, in_qsize 8, out_qsize 0\n",
            "2019-07-31 12:39:32,758 : INFO : EPOCH 4 - PROGRESS: at 98.46% examples, 403214 words/s, in_qsize 5, out_qsize 0\n",
            "2019-07-31 12:39:32,787 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-07-31 12:39:32,797 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-07-31 12:39:32,818 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-07-31 12:39:32,848 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-07-31 12:39:32,850 : INFO : EPOCH - 4 : training on 2988089 raw words (2494105 effective words) took 6.2s, 403032 effective words/s\n",
            "2019-07-31 12:39:33,864 : INFO : EPOCH 5 - PROGRESS: at 15.66% examples, 393470 words/s, in_qsize 8, out_qsize 0\n",
            "2019-07-31 12:39:34,875 : INFO : EPOCH 5 - PROGRESS: at 32.49% examples, 405297 words/s, in_qsize 8, out_qsize 0\n",
            "2019-07-31 12:39:35,877 : INFO : EPOCH 5 - PROGRESS: at 49.68% examples, 413452 words/s, in_qsize 8, out_qsize 0\n",
            "2019-07-31 12:39:36,896 : INFO : EPOCH 5 - PROGRESS: at 65.72% examples, 407724 words/s, in_qsize 8, out_qsize 0\n",
            "2019-07-31 12:39:37,902 : INFO : EPOCH 5 - PROGRESS: at 82.36% examples, 408556 words/s, in_qsize 8, out_qsize 0\n",
            "2019-07-31 12:39:38,838 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-07-31 12:39:38,854 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-07-31 12:39:38,864 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-07-31 12:39:38,883 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-07-31 12:39:38,884 : INFO : EPOCH - 5 : training on 2988089 raw words (2494322 effective words) took 6.0s, 413663 effective words/s\n",
            "2019-07-31 12:39:38,885 : INFO : training on a 14940445 raw words (12471858 effective words) took 30.6s, 407460 effective words/s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt4mmhTUE5we",
        "colab_type": "code",
        "colab": {},
        "outputId": "5527ec3f-0f4f-4b24-e596-3f699ab361f3"
      },
      "source": [
        "# 모델의 하이퍼파라미터를 설정한 내용을 모델 이름에 담는다.\n",
        "# 모델을 저장하면 Word2Vec.load()를 통해 모델을 다시 사용할 수 있다.\n",
        "model_name = \"300features_40minwords_10context\"\n",
        "model.save(model_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-07-31 12:41:53,619 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
            "2019-07-31 12:41:53,620 : INFO : not storing attribute vectors_norm\n",
            "2019-07-31 12:41:53,622 : INFO : not storing attribute cum_table\n",
            "2019-07-31 12:41:53,623 : WARNING : this function is deprecated, use smart_open.open instead\n",
            "2019-07-31 12:41:53,856 : INFO : saved 300features_40minwords_10context\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hwqzpSPE5wi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_features(words, model, num_features):\n",
        "    # 출력 벡터 초기화\n",
        "    feature_vector = np.zeros((num_features), dtype=np.float32)\n",
        "    \n",
        "    num_words = 0\n",
        "    # 어휘사전 준비\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    \n",
        "    for w in words:\n",
        "        if w in index2word_set:\n",
        "            num_words += 1\n",
        "            # 사전에 해당하는 단어에 대해 단어 벡터를 더함\n",
        "            feature_vector = np.add(feature_vector, model[w])\n",
        "            \n",
        "    # 문장의 단어 수만큼 나누어 단어 벡터의 평균값을 문장 벡터로 함\n",
        "    feature_vector = np.divide(feature_vector, num_words)\n",
        "    return feature_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lzKLUawE5wk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dataset(reviews, model, num_features):\n",
        "    dataset = list()\n",
        "    \n",
        "    for s in reviews:\n",
        "        dataset.append(get_features(s, model, num_features))\n",
        "    \n",
        "    reviewFeatureVecs = np.stack(dataset)\n",
        "    \n",
        "    return reviewFeatureVecs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-o3IbzJE5wn",
        "colab_type": "code",
        "colab": {},
        "outputId": "4db876b0-5bbd-4932-8211-c8cfa78e5e9e"
      },
      "source": [
        "train_data_vecs = get_dataset(sentences, model, num_features)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO-NoBe8E5wp",
        "colab_type": "text"
      },
      "source": [
        "# 학습과 검증  데이터셋 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK9wy-s8E5wq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = train_data_vecs\n",
        "y = np.array(sentiment)\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "TEST_SPLIT = 0.2\n",
        "\n",
        "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RANDOM_SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx24BV_PE5wt",
        "colab_type": "text"
      },
      "source": [
        "# 모델 선언 및 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLSXSeURE5wt",
        "colab_type": "code",
        "colab": {},
        "outputId": "514339d8-5700-4773-b6f3-9acb55ba9532"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lgs = LogisticRegression(class_weight='balanced')\n",
        "lgs.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
              "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
              "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
              "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3xVe_AsE5ww",
        "colab_type": "text"
      },
      "source": [
        "# 검증 데이터로 성능 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHDADdAlE5wx",
        "colab_type": "code",
        "colab": {},
        "outputId": "eb84f527-ecd7-448d-c237-4caff9284a7c"
      },
      "source": [
        "print(\"Accuracy: %f\" % lgs.score(X_eval, y_eval))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.863000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v5-k5-0E5w0",
        "colab_type": "text"
      },
      "source": [
        "# 캐글에 데이터 제출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZrPW4t_E5w1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEST_CLEAN_DATA = 'test_clean.csv'\n",
        "\n",
        "test_data = pd.read_csv(DATA_IN_PATH+TEST_CLEAN_DATA)\n",
        "\n",
        "test_review = list(test_data['review'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSHxJfhRE5w5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sentences = list()\n",
        "for review in test_review:\n",
        "    test_sentences.append(review.split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE04ctODE5w7",
        "colab_type": "code",
        "colab": {},
        "outputId": "9a2b590b-c4db-4f48-fd2f-bedddfae8750"
      },
      "source": [
        "test_data_vecs = get_dataset(test_sentences, model, num_features)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8KqqsIYE5w-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_OUT_PATH = r'./data_out/'\n",
        "\n",
        "test_predicted = lgs.predict(test_data_vecs)\n",
        "\n",
        "if not os.path.exists(DATA_OUT_PATH):\n",
        "    os.makedirs(DATA_OUT_PATH)\n",
        "    \n",
        "ids = list(test_data['id'])\n",
        "\n",
        "answer_dataset = pd.DataFrame({'id': ids, 'sentiment': test_predicted})\n",
        "answer_dataset.to_csv(DATA_OUT_PATH+'lgs_w2v_answer.csv', index=False, quoting=3)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}